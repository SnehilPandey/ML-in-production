# Databricks notebook source
# MAGIC %md-sandbox
# MAGIC 
# MAGIC <div style="text-align: center; line-height: 0; padding-top: 9px;">
# MAGIC   <img src="https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png" alt="Databricks Learning" style="width: 600px">
# MAGIC </div>

# COMMAND ----------

# MAGIC %md # Batch Deployment
# MAGIC 
# MAGIC Batch inference is the most common way of deploying machine learning models.  This lesson introduces various strategies for deploying models using batch including Spark, write optimizations, and on the JVM.
# MAGIC 
# MAGIC ## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>
# MAGIC  - Explore batch deployment options
# MAGIC  - Apply an **`sklearn`** model to a Spark DataFrame and save the results
# MAGIC  - Employ write optimizations including partitioning and Z-ordering
# MAGIC  - Compare other batch deployment options

# COMMAND ----------

# MAGIC %run ../Includes/Classroom-Setup

# COMMAND ----------

# MAGIC %md
# MAGIC ### Inference in Batch
# MAGIC 
# MAGIC Batch deployment represents the vast majority of use cases for deploying machine learning models.<br><br>
# MAGIC 
# MAGIC * This normally means running the predictions from a model and saving them somewhere for later use.
# MAGIC * For live serving, results are often saved to a database that will serve the saved prediction quickly. 
# MAGIC * In other cases, such as populating emails, they can be stored in less performant data stores such as a blob store.
# MAGIC 
# MAGIC <img src="https://files.training.databricks.com/images/eLearning/ML-Part-4/batch-predictions.png" width=800px />
# MAGIC 
# MAGIC Writing the results of an inference can be optimized in a number of ways...<br><br>
# MAGIC 
# MAGIC * For large amounts of data, predictions and writes should be performed in parallel
# MAGIC * **The access pattern for the saved predictions should also be kept in mind in how the data is written**
# MAGIC   - For static files or data warehouses, partitioning speeds up data reads
# MAGIC   - For databases, indexing the database on the relevant query generally improves performance
# MAGIC   - In either case, the index is working similar to an index in a book: it allows you to skip ahead to the relevant content

# COMMAND ----------

# MAGIC %md
# MAGIC There are a few other considerations to ensure the accuracy of a model...<br><br>
# MAGIC 
# MAGIC * First is to make sure that the model matches expectations
# MAGIC   - We'll cover this in further detail in the model drift section
# MAGIC * Second is to **retrain the model on the majority of your dataset**
# MAGIC   - Either use the entire dataset for training or around 95% of it
# MAGIC   - A train/test split is a good method in tuning hyperparameters and estimating how the model will perform on unseen data
# MAGIC   - Retraining the model on the majority of the dataset ensures that you have as much information as possible factored into the model

# COMMAND ----------

# MAGIC %md ### Inference in Spark
# MAGIC 
# MAGIC Models trained in various machine learning libraries can be applied at scale using Spark.  To do this, use **`mlflow.pyfunc.spark_udf`** and pass in the **`SparkSession`**, name of the model, and run id.
# MAGIC 
# MAGIC <img src="https://files.training.databricks.com/images/icon_note_24.png"/> Using UDF's in Spark means that supporting libraries must be installed on every node in the cluster.  In the case of **`sklearn`**, this is installed in Databricks clusters by default.  When using other libraries, you will need to install them to ensure that they will work as UDFs.  

# COMMAND ----------

# MAGIC %md Start by training an **`sklearn`** model.  Apply it using a Spark UDF generated by **`mlflow`**.
# MAGIC 
# MAGIC Import the data.  **Do not perform a train/test split.**
# MAGIC 
# MAGIC <img src="https://files.training.databricks.com/images/icon_note_24.png"/> It is common to skip the train/test split in training a final model.

# COMMAND ----------

import pandas as pd

df = pd.read_parquet("/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.parquet")
X = df.drop(["price"], axis=1)
y = df["price"]

# COMMAND ----------

# MAGIC %md Train and log model

# COMMAND ----------

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import mlflow.sklearn

with mlflow.start_run(run_name="Final RF Model") as run:
    rf = RandomForestRegressor(n_estimators=100, max_depth=5)
    rf.fit(X, y)

    predictions = rf.predict(X)
    mlflow.sklearn.log_model(rf, "random_forest_model")

    mse = mean_squared_error(y, predictions) # This is on the same data the model was trained
    mlflow.log_metric("mse", mse)

# COMMAND ----------

# MAGIC %md Create a Spark DataFrame from the Pandas DataFrame.

# COMMAND ----------

spark_df = spark.createDataFrame(X)
display(spark_df)

# COMMAND ----------

# MAGIC %md MLflow easily produces a Spark user defined function (UDF).  This bridges the gap between Python environments and applying models at scale using Spark.

# COMMAND ----------

predict = mlflow.pyfunc.spark_udf(spark, f"runs:/{run.info.run_id}/random_forest_model")

# COMMAND ----------

# MAGIC %md Apply the model as a standard UDF using the column names as the input to the function.

# COMMAND ----------

prediction_df = spark_df.withColumn("prediction", predict(*spark_df.columns))

display(prediction_df)

# COMMAND ----------

# MAGIC %md ### Write Optimizations
# MAGIC 
# MAGIC There are many possible optimizations depending on your batch deployment scenerio.  In Spark and Delta Lake, the following optimizations are possible:<br><br>
# MAGIC 
# MAGIC - **Partitioning:** stores data associated with different categorical values in different directories
# MAGIC - **Z-Ordering:** colocates related information in the same set of files
# MAGIC - **Data Skipping:** aims at speeding up queries that contain filters (WHERE clauses)
# MAGIC - **Partition Pruning:** speeds up queries by limiting the amount of data read
# MAGIC 
# MAGIC Other optimizations include:<br><br>
# MAGIC 
# MAGIC - **Database indexing:** allows certain table columns to be more effectively queried 
# MAGIC - **Geo-replication:** replicates data in different geographical regions

# COMMAND ----------

# MAGIC %md Partition by neighborhood.

# COMMAND ----------

dbutils.fs.rm(f"{working_dir}/batch-predictions-partitioned.delta", recurse=True)
delta_partitioned_path = f"{working_dir}/batch-predictions-partitioned.delta"

prediction_df.write.partitionBy("neighbourhood_cleansed").mode("OVERWRITE").format("delta").save(delta_partitioned_path)

# COMMAND ----------

# MAGIC %md Take a look at the files.

# COMMAND ----------

display(dbutils.fs.ls(delta_partitioned_path))

# COMMAND ----------

# MAGIC %md Z-Ordering is a form of multi-dimensional clustering that colocates related information in the same set of files.  It reduces the amount of data that needs to be read.  <a href="https://docs.databricks.com/delta/optimizations/file-mgmt.html#z-ordering-multi-dimensional-clustering" target="_blank">You can read more about it here.</a> Let's z-order by zipcode.

# COMMAND ----------

spark.sql(f"OPTIMIZE delta.`{delta_partitioned_path}` ZORDER BY (zipcode)")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Feature Store Batch Scoring

# COMMAND ----------

# MAGIC %md
# MAGIC Create feature table
# MAGIC 
# MAGIC <img src="https://files.training.databricks.com/images/icon_note_24.png"/> Read dataframe from the csv file with spark directly to track **`data source`** in Feature Store

# COMMAND ----------

from databricks import feature_store
from databricks.feature_store import feature_table,FeatureLookup

## create a feature store client
fs = feature_store.FeatureStoreClient()

# COMMAND ----------

from pyspark.sql.functions import monotonically_increasing_id

## build feature dataframe, add index column and drop label
df = (spark
      .read
      .csv("/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv", header=True, inferSchema=True)
      .withColumn("index", monotonically_increasing_id())
     )
## feature data - all the columns except for the true label
features_df = df.drop("price")

## inference data - contains only index and label columns, if you have online features, it should be added to inference_df as well
inference_df = df.select("index", "price")

# COMMAND ----------

import uuid
## create unique table name
spark.sql(f"CREATE DATABASE IF NOT EXISTS {clean_username}")
feature_table_name = f"{clean_username}.airbnb_fs_" + str(uuid.uuid4())[:6]
print(feature_table_name)

## create feature table
fs.create_table(
    name=feature_table_name,
    primary_keys=["index"],
    df=features_df,
    description="review cols of Airbnb data"
)

# COMMAND ----------

# MAGIC %md
# MAGIC Create training set from feature store using **`fs.create_training_set`**

# COMMAND ----------

## FeatureLoopup object
#feature_lookups = [FeatureLookup(feature_table_name, f, "index") for f in features_df.columns if f!="index"] ## exclude index colum
## uncomment the command below to create lookup features if using Runtime 9.1 ML
feature_lookups = [FeatureLookup(table_name = feature_table_name, feature_names = None, lookup_key = "index") ]

## fs.create_training_set will look up features in model_feature_lookups with matched key from inference_data_df
training_set = fs.create_training_set(inference_df, feature_lookups, label="price", exclude_columns="index")

# COMMAND ----------

# MAGIC %md
# MAGIC Log a feature store packaged model.

# COMMAND ----------

## create unique model name
model_name = f"{clean_username}_airbnb_fs_model_" + str(uuid.uuid4())[:6]
model_name

# COMMAND ----------

from mlflow.models.signature import infer_signature
## log RF model as a feature store packaged model and register the packaged model in model registry as `model_name`
fs.log_model(
    model=rf,
    artifact_path="feature_store_model",
    flavor=mlflow.sklearn,
    training_set=training_set,
    registered_model_name=model_name,
    input_example=X[:5],
    signature=infer_signature(X, y)
)

# COMMAND ----------

# MAGIC %md
# MAGIC Now view the model on the Model Registry UI.
# MAGIC 
# MAGIC <img src="https://files.training.databricks.com/images/mlflow/FS_MLProd_model_registry.png" alt="step12" width="800"/>
# MAGIC 
# MAGIC </br>
# MAGIC </br>
# MAGIC Meanwhile you can find the registered model appears also on the Feature Store UI.
# MAGIC 
# MAGIC <img src="https://files.training.databricks.com/images/mlflow/FS_MLProd_feature_store_UI.png" alt="step12" width="800"/>

# COMMAND ----------

# MAGIC %md
# MAGIC batch score with the feature store model

# COMMAND ----------

## for simplicity sake, we will just predict on the same inference_data_df
batch_input_df = inference_df.drop("price") #exclude true label
with_predictions = fs.score_batch(f"models:/{model_name}/1", 
                                  batch_input_df, result_type='double')
display(with_predictions)

# COMMAND ----------

# MAGIC %md ### Other Deployment Options
# MAGIC 
# MAGIC There are a number of other common batch deployment options.  One common use case is going from a Python environment for training to a Java environment for deployment.  Here are a few tools that can help with that:<br><br>
# MAGIC  - **An Easy Port to Java:** In certain models, such as linear regression, the coefficients of a trained model can be taken and implemented by hand in Java.  This can work with tree-based models as well.
# MAGIC  - **Re-serializing for Java:** Since Python uses Pickle by default to serialize, a library like <a href="https://github.com/jpmml/jpmml-sklearn" target="_blank">jpmml-sklearn</a> can de-serialize **`sklearn`** libraries and re-serialize them for use in Java environments.
# MAGIC  - **Leveraging Library Functionality:** Some libraries include the ability to deploy to Java such as <a href="https://github.com/dmlc/xgboost/tree/master/jvm-packages" target="_blank">xgboost4j</a>.
# MAGIC  - **Containers:** Containerized solutions are becoming increasingly popular since they offer the encapsulation and reliability offered by jars while offering more deployment options than just the Java environment.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Review
# MAGIC **Question:** What are the main considerations in batch deployments?  
# MAGIC **Answer:** The following considerations help determine the best way to deploy batch inference results:
# MAGIC * How the data will be queried
# MAGIC * How the data will be written
# MAGIC * The training and deployment environment
# MAGIC * What data the final model is trained on
# MAGIC 
# MAGIC **Question:** How can you optimize inference reads and writes?  
# MAGIC **Answer:** Writes can be optimized by managing parallelism.  In Spark, this would mean managing the partitions of a DataFrame such that work is evenly distributed and you have the most efficient connections back to the target database.

# COMMAND ----------

# MAGIC %md
# MAGIC ## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Lab<br>
# MAGIC 
# MAGIC Start the labs for this lesson, [Batch Lab]($./Labs/01-Batch-Lab)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Additional Topics & Resources
# MAGIC 
# MAGIC **Q:** Where can I find more information on UDF's created by MLflow?  
# MAGIC **A:** See the <a href="https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html" target="_blank">MLflow documentation for details</a>

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC &copy; 2022 Databricks, Inc. All rights reserved.<br/>
# MAGIC Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href="https://www.apache.org/">Apache Software Foundation</a>.<br/>
# MAGIC <br/>
# MAGIC <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a> | <a href="https://help.databricks.com/">Support</a>
